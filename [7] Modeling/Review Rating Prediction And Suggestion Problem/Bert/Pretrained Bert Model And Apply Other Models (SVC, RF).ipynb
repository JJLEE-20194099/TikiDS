{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Bert_review",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1s_K9KV0MKNN",
        "outputId": "0281479d-469e-4fed-a531-b3e53666771c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'transformers' already exists and is not an empty directory.\n",
            "/content/transformers\n",
            "Processing /content/transformers\n",
            "\u001b[33m  DEPRECATION: A future pip version will change local packages to be built in-place without first copying to a temporary directory. We recommend you use --use-feature=in-tree-build to test your packages with this new behavior before it becomes the default.\n",
            "   pip 21.3 will remove support for this functionality. You can find discussion regarding this at https://github.com/pypa/pip/issues/7555.\u001b[0m\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers==4.16.0.dev0) (4.8.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.16.0.dev0) (2019.12.20)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers==4.16.0.dev0) (0.0.46)\n",
            "Requirement already satisfied: tokenizers>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers==4.16.0.dev0) (0.11.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.16.0.dev0) (3.4.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.16.0.dev0) (1.19.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==4.16.0.dev0) (4.62.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers==4.16.0.dev0) (21.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.16.0.dev0) (2.23.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers==4.16.0.dev0) (0.2.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers==4.16.0.dev0) (6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers==4.16.0.dev0) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers==4.16.0.dev0) (3.0.6)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers==4.16.0.dev0) (3.6.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.16.0.dev0) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.16.0.dev0) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.16.0.dev0) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.16.0.dev0) (2.10)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.16.0.dev0) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.16.0.dev0) (1.1.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.16.0.dev0) (1.15.0)\n",
            "Building wheels for collected packages: transformers\n",
            "  Building wheel for transformers (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for transformers: filename=transformers-4.16.0.dev0-py3-none-any.whl size=3379183 sha256=9ccb7a4c0b52622dafa93c98f4f9ddbc6fdd1452f89e2194087de204093e3b45\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-uxtpaomg/wheels/49/62/f4/6730819eed4e6468662b1519bf3bf46419b2335990c77f8767\n",
            "Successfully built transformers\n",
            "Installing collected packages: transformers\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.16.0.dev0\n",
            "    Uninstalling transformers-4.16.0.dev0:\n",
            "      Successfully uninstalled transformers-4.16.0.dev0\n",
            "Successfully installed transformers-4.16.0.dev0\n",
            "/content\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/huggingface/transformers.git\n",
        "%cd transformers\n",
        "!pip3 install --upgrade .\n",
        "%cd ../"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install underthesea\n",
        "!pip3 install vncorenlp\n",
        "\n",
        "!mkdir -p vncorenlp/models/wordsegmenter\n",
        "!wget https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/VnCoreNLP-1.1.1.jar\n",
        "!wget https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/models/wordsegmenter/vi-vocab\n",
        "!wget https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/models/wordsegmenter/wordsegmenter.rdr\n",
        "!mv VnCoreNLP-1.1.1.jar vncorenlp/ \n",
        "!mv vi-vocab vncorenlp/models/wordsegmenter/\n",
        "!mv wordsegmenter.rdr vncorenlp/models/wordsegmenter/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BH-cbeyzMQ3X",
        "outputId": "2e5e6e8e-a64b-45df-cc4f-08cc31ef1695"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: underthesea in /usr/local/lib/python3.7/dist-packages (1.3.3)\n",
            "Requirement already satisfied: Click>=6.0 in /usr/local/lib/python3.7/dist-packages (from underthesea) (7.1.2)\n",
            "Requirement already satisfied: unidecode in /usr/local/lib/python3.7/dist-packages (from underthesea) (1.3.2)\n",
            "Requirement already satisfied: torch>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from underthesea) (1.10.0+cu111)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from underthesea) (2.23.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from underthesea) (1.0.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from underthesea) (4.62.3)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from underthesea) (6.0)\n",
            "Requirement already satisfied: python-crfsuite>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from underthesea) (0.9.7)\n",
            "Requirement already satisfied: transformers>=3.5.0 in /usr/local/lib/python3.7/dist-packages (from underthesea) (4.16.0.dev0)\n",
            "Requirement already satisfied: seqeval in /usr/local/lib/python3.7/dist-packages (from underthesea) (1.2.2)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from underthesea) (3.2.5)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from underthesea) (1.1.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.1.0->underthesea) (3.10.0.2)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers>=3.5.0->underthesea) (4.8.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers>=3.5.0->underthesea) (2019.12.20)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers>=3.5.0->underthesea) (21.3)\n",
            "Requirement already satisfied: tokenizers>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers>=3.5.0->underthesea) (0.11.1)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers>=3.5.0->underthesea) (0.0.46)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers>=3.5.0->underthesea) (0.2.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers>=3.5.0->underthesea) (3.4.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers>=3.5.0->underthesea) (1.19.5)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers>=3.5.0->underthesea) (3.0.6)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers>=3.5.0->underthesea) (3.6.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk->underthesea) (1.15.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->underthesea) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->underthesea) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->underthesea) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->underthesea) (3.0.4)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->underthesea) (1.4.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->underthesea) (3.0.0)\n",
            "Requirement already satisfied: vncorenlp in /usr/local/lib/python3.7/dist-packages (1.0.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from vncorenlp) (2.23.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->vncorenlp) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->vncorenlp) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->vncorenlp) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->vncorenlp) (2021.10.8)\n",
            "--2022-01-04 09:45:55--  https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/VnCoreNLP-1.1.1.jar\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 27412575 (26M) [application/octet-stream]\n",
            "Saving to: ‘VnCoreNLP-1.1.1.jar’\n",
            "\n",
            "VnCoreNLP-1.1.1.jar 100%[===================>]  26.14M  92.5MB/s    in 0.3s    \n",
            "\n",
            "2022-01-04 09:45:55 (92.5 MB/s) - ‘VnCoreNLP-1.1.1.jar’ saved [27412575/27412575]\n",
            "\n",
            "--2022-01-04 09:45:55--  https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/models/wordsegmenter/vi-vocab\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.109.133, 185.199.111.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 526544 (514K) [application/octet-stream]\n",
            "Saving to: ‘vi-vocab’\n",
            "\n",
            "vi-vocab            100%[===================>] 514.20K  --.-KB/s    in 0.04s   \n",
            "\n",
            "2022-01-04 09:45:56 (11.6 MB/s) - ‘vi-vocab’ saved [526544/526544]\n",
            "\n",
            "--2022-01-04 09:45:56--  https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master/models/wordsegmenter/wordsegmenter.rdr\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.111.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 128508 (125K) [text/plain]\n",
            "Saving to: ‘wordsegmenter.rdr’\n",
            "\n",
            "wordsegmenter.rdr   100%[===================>] 125.50K  --.-KB/s    in 0.02s   \n",
            "\n",
            "2022-01-04 09:45:56 (5.96 MB/s) - ‘wordsegmenter.rdr’ saved [128508/128508]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import os\n",
        "import re\n",
        "from underthesea import word_tokenize"
      ],
      "metadata": {
        "id": "KNRu2gtRMdZG"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TFAutoModel, AutoTokenizer\n",
        "phobert = TFAutoModel.from_pretrained(\"vinai/phobert-base\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"vinai/phobert-base\", use_fast=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5iQJ4qrzMgLG",
        "outputId": "b1fec31e-4593-44b4-9ab5-2b20a385e1fb"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at vinai/phobert-base were not used when initializing TFRobertaModel: ['lm_head']\n",
            "- This IS expected if you are initializing TFRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFRobertaModel were initialized from the model checkpoint at vinai/phobert-base.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.pad_token_id"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x8XiEfQlMh-G",
        "outputId": "8f4819ad-ee5f-4f31-b0be-a64fad04951a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.cls_token_id"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ki_VZPkqMjmf",
        "outputId": "a8aace05-9bf3-45ff-99eb-46331e43d226"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('./drive/MyDrive/modified_rating_list.csv', sep=',', encoding='utf-8')"
      ],
      "metadata": {
        "id": "K-z6_uAwMqh-"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "EpnWNjs5NF52",
        "outputId": "c76629a2-a09e-418c-f4f6-440bfd146237"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-8b5d1f77-ab73-4185-8d7d-53145f86a319\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>comment</th>\n",
              "      <th>rating</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>treo đầu dê thịt chó hình_ảnh thùng sữa yomost...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>tuyệt với shipper tuyệt_vời bao shipper tâm ng...</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>sữa mẹ uống hộp ói mữa ngày tiêu tức ngực chịu...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>tắc_trách sữa yomost uống bé ỉa_chảy ói mửa lu...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>theo sưa hộp lit ngon hơn cảm_giác hộp sữa băn...</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8b5d1f77-ab73-4185-8d7d-53145f86a319')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-8b5d1f77-ab73-4185-8d7d-53145f86a319 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-8b5d1f77-ab73-4185-8d7d-53145f86a319');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                                             comment  rating\n",
              "0  treo đầu dê thịt chó hình_ảnh thùng sữa yomost...       1\n",
              "1  tuyệt với shipper tuyệt_vời bao shipper tâm ng...       5\n",
              "2  sữa mẹ uống hộp ói mữa ngày tiêu tức ngực chịu...       2\n",
              "3  tắc_trách sữa yomost uống bé ỉa_chảy ói mửa lu...       1\n",
              "4  theo sưa hộp lit ngon hơn cảm_giác hộp sữa băn...       4"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from vncorenlp import VnCoreNLP\n",
        "rdrsegmenter = VnCoreNLP(\"vncorenlp/VnCoreNLP-1.1.1.jar\", annotators=\"wseg\", max_heap_size='-Xmx500m') \n",
        "\n",
        "def remove_punctuation(text):\n",
        "    text = re.sub(r\"[\\.,\\?]+$-\", \"\", text)\n",
        "    text = text.replace(\",\", \" \").replace(\".\",\" \") \\\n",
        "        .replace(\";\", \" \").replace(\"“\", \" \") \\\n",
        "        .replace(\":\", \" \").replace(\"”\", \" \") \\\n",
        "        .replace('\"', \" \").replace(\"'\", \" \") \\\n",
        "        .replace(\"!\", \" \").replace(\"?\", \" \") \\\n",
        "        .replace(\"-\", \" \").replace(\"?\", \" \")\n",
        " \n",
        "    text = text.strip() \n",
        "\n",
        "    return text\n",
        "\n",
        "def word_segment(text): \n",
        "    sentences = rdrsegmenter.tokenize(text) \n",
        "    text = \"\"\n",
        "    for sentence in sentences:\n",
        "        text += \" \" + \" \".join(sentence)\n",
        "    return text.strip()\n",
        "\n",
        "def filter_spam_word(text, replace_dict): \n",
        "    words = text.split()\n",
        "    result = []\n",
        "    for word in words:\n",
        "        if replace_dict.get(word, None):\n",
        "            word = replace_dict[word]\n",
        "        result.append(word)\n",
        "    \n",
        "    if (\" \".join(result).count(\"<unk>\") == len(words)):\n",
        "        return \"\"\n",
        "    return text "
      ],
      "metadata": {
        "id": "zMA3xV7BM0sy"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.dropna(subset=['comment', 'rating'], inplace=True)"
      ],
      "metadata": {
        "id": "3nvM_kXHOIZz"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = df['comment'].to_numpy()\n"
      ],
      "metadata": {
        "id": "Dag1TwYZNCeg"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y = df['rating'].apply(lambda x: int(x))"
      ],
      "metadata": {
        "id": "B06ScwABN-Nx"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import pickle\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer"
      ],
      "metadata": {
        "id": "Jguog_3hNyuz"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=1)"
      ],
      "metadata": {
        "id": "EQauhVbPNbxf"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "extractor = TfidfVectorizer(min_df=5, max_df=0.8, max_features=768)\n",
        "extractor.fit(X_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LhLR1tzQOC3M",
        "outputId": "d502d83e-f327-4bb9-d918-ef855546c451"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TfidfVectorizer(max_df=0.8, max_features=768, min_df=5)"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train =  extractor.transform(X_train)\n",
        "X_val = extractor.transform(X_val)\n",
        "X_test = extractor.transform(X_test)"
      ],
      "metadata": {
        "id": "MkWd2Os4Or8E"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = LinearSVC(C = 0.05,max_iter=1000)\n",
        "model.fit(X_train, (y_train-1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s6dXqx1cOT8U",
        "outputId": "69b33127-4276-4844-aa11-ff3372b6f9ef"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LinearSVC(C=0.05)"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "acc_train = model.score(X_train, y_train-1)\n",
        "acc_val = model.score(X_val, y_val-1)\n",
        "acc_test = model.score(X_test, y_test-1)\n",
        "\n",
        "print(acc_train)\n",
        "print(acc_val)\n",
        "print(acc_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aPAin6V-OFcJ",
        "outputId": "b52bd6ad-94b0-4342-e5b0-affbc004c738"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.7040686456832024\n",
            "0.70139591106152\n",
            "0.7005002510553624\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = LogisticRegression(C = 1,max_iter=3000, multi_class='multinomial',solver='lbfgs')\n",
        "model.fit(X_train, (y_train-1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Axzcs-JIPZ7B",
        "outputId": "b71536ad-9209-40a2-dcdb-e0d1d0372ec6"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LogisticRegression(C=1, max_iter=3000, multi_class='multinomial')"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "acc_train = model.score(X_train, y_train-1)\n",
        "acc_val = model.score(X_val, y_val-1)\n",
        "acc_test = model.score(X_test, y_test-1)\n",
        "\n",
        "print(acc_train)\n",
        "print(acc_val)\n",
        "print(acc_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tGZ6o865Pbho",
        "outputId": "faac31b9-6c0c-4bab-a678-5715d2094194"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.7096709516946395\n",
            "0.7054406824970653\n",
            "0.7056236401167872\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# model = RandomForestClassifier()\n",
        "# model = model.fit(X_train, y_train-1)"
      ],
      "metadata": {
        "id": "uewtYSOAPdBo"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.models.Sequential([\n",
        "    #tf.keras.layers.Input(),\n",
        "    tf.keras.layers.Dense(256, activation='relu', kernel_regularizer=tf.keras.regularizers.L2(0.0)),\n",
        "    tf.keras.layers.Dense(256, activation='relu', kernel_regularizer=tf.keras.regularizers.L2(0.0)),\n",
        "    tf.keras.layers.Dropout(0.5),\n",
        "    tf.keras.layers.Dense(5, activation ='softmax', kernel_regularizer=tf.keras.regularizers.L2(0.0))\n",
        "])\n",
        "\n",
        "model.compile(optimizer = tf.keras.optimizers.Adam(1e-4),\n",
        "              loss=tf.keras.losses.sparse_categorical_crossentropy,\n",
        "              metrics=['acc'])"
      ],
      "metadata": {
        "id": "g7g23pxzPlZK"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if not os.path.exists(\"models\"):\n",
        "    os.makedirs(\"models\")\n",
        "\n",
        "mcp_save = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=\"models/tfidf_nn_classify_model.h5\",\n",
        "    save_weights_only=True,\n",
        "    monitor='val_acc',\n",
        "    mode='max',\n",
        "    save_best_only=True)  \n",
        "\n",
        "model.fit(X_train.toarray(), y_train-1,\n",
        "          epochs=50,\n",
        "          batch_size=32,\n",
        "          validation_data=(X_val.toarray(), y_val-1),\n",
        "          callbacks=[mcp_save])"
      ],
      "metadata": {
        "id": "Y8TzLLwHPp3P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "35b7c26b-8a2c-437d-bd28-bc20098a7b0b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "10755/10755 [==============================] - 46s 4ms/step - loss: 0.8529 - acc: 0.6969 - val_loss: 0.8088 - val_acc: 0.7073\n",
            "Epoch 2/50\n",
            "10755/10755 [==============================] - 44s 4ms/step - loss: 0.8034 - acc: 0.7115 - val_loss: 0.8017 - val_acc: 0.7101\n",
            "Epoch 3/50\n",
            "10755/10755 [==============================] - 45s 4ms/step - loss: 0.7917 - acc: 0.7154 - val_loss: 0.7971 - val_acc: 0.7117\n",
            "Epoch 4/50\n",
            "10755/10755 [==============================] - 45s 4ms/step - loss: 0.7811 - acc: 0.7186 - val_loss: 0.7940 - val_acc: 0.7129\n",
            "Epoch 5/50\n",
            "10755/10755 [==============================] - 45s 4ms/step - loss: 0.7710 - acc: 0.7216 - val_loss: 0.7918 - val_acc: 0.7140\n",
            "Epoch 6/50\n",
            "10755/10755 [==============================] - 45s 4ms/step - loss: 0.7602 - acc: 0.7252 - val_loss: 0.7919 - val_acc: 0.7144\n",
            "Epoch 7/50\n",
            "10755/10755 [==============================] - 49s 5ms/step - loss: 0.7494 - acc: 0.7288 - val_loss: 0.7949 - val_acc: 0.7130\n",
            "Epoch 8/50\n",
            "10755/10755 [==============================] - 47s 4ms/step - loss: 0.7373 - acc: 0.7325 - val_loss: 0.7973 - val_acc: 0.7131\n",
            "Epoch 9/50\n",
            "10755/10755 [==============================] - 80s 7ms/step - loss: 0.7251 - acc: 0.7367 - val_loss: 0.8021 - val_acc: 0.7121\n",
            "Epoch 10/50\n",
            "10755/10755 [==============================] - 51s 5ms/step - loss: 0.7127 - acc: 0.7411 - val_loss: 0.8116 - val_acc: 0.7113\n",
            "Epoch 11/50\n",
            "10755/10755 [==============================] - 45s 4ms/step - loss: 0.7011 - acc: 0.7455 - val_loss: 0.8160 - val_acc: 0.7090\n",
            "Epoch 12/50\n",
            "10755/10755 [==============================] - 51s 5ms/step - loss: 0.6885 - acc: 0.7496 - val_loss: 0.8247 - val_acc: 0.7070\n",
            "Epoch 13/50\n",
            "10755/10755 [==============================] - 49s 5ms/step - loss: 0.6763 - acc: 0.7543 - val_loss: 0.8371 - val_acc: 0.7049\n",
            "Epoch 14/50\n",
            "10755/10755 [==============================] - 49s 5ms/step - loss: 0.6640 - acc: 0.7590 - val_loss: 0.8512 - val_acc: 0.7035\n",
            "Epoch 15/50\n",
            "10755/10755 [==============================] - 49s 5ms/step - loss: 0.6524 - acc: 0.7627 - val_loss: 0.8634 - val_acc: 0.7007\n",
            "Epoch 16/50\n",
            "10755/10755 [==============================] - 49s 5ms/step - loss: 0.6410 - acc: 0.7669 - val_loss: 0.8779 - val_acc: 0.7018\n",
            "Epoch 17/50\n",
            "10755/10755 [==============================] - 49s 5ms/step - loss: 0.6302 - acc: 0.7716 - val_loss: 0.8940 - val_acc: 0.6950\n",
            "Epoch 18/50\n",
            "10755/10755 [==============================] - 45s 4ms/step - loss: 0.6196 - acc: 0.7752 - val_loss: 0.9066 - val_acc: 0.6952\n",
            "Epoch 19/50\n",
            " 1416/10755 [==>...........................] - ETA: 33s - loss: 0.5927 - acc: 0.7858"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_weights(\"models/tfidf_nn_classify_model.h5\")\n",
        "model.evaluate(X_train.toarray(), y_train-1)\n",
        "model.evaluate(X_val.toarray(), y_val-1)\n",
        "model.evaluate(X_test.toarray(), y_test-1)"
      ],
      "metadata": {
        "id": "fOsTU4FtPsex"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_tokenized = [tokenizer.encode(sent) for sent in X] \n",
        "\n",
        "max_length = 0\n",
        "for sent in X_tokenized:\n",
        "    if max_length < len(sent):\n",
        "        max_length = len(sent)\n",
        "\n",
        "if max_length > 256: \n",
        "    max_length = 256 \n",
        "truncated_X_tokenized = [(sent[:max_length-1] + [tokenizer.eos_token_id]) if (len(sent) > max_length) else sent for sent in X_tokenized]\n",
        "padded_X_tokenized = np.array([sent + [tokenizer.pad_token_id]*(max_length-len(sent)) for sent in truncated_X_tokenized]) \n",
        "attention_mask = np.where(padded_X_tokenized == tokenizer.pad_token_id , 0 , 1)\n",
        "\n",
        "print(padded_X_tokenized.shape)\n",
        "print(attention_mask.shape)"
      ],
      "metadata": {
        "id": "cdyLhFJTfUwj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label = np.array(df['rating'].tolist())"
      ],
      "metadata": {
        "id": "36cevi-9gB5h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = tf.data.Dataset.from_tensor_slices((padded_X_tokenized, attention_mask, label)).batch(32) \n",
        "print(tf.data.experimental.cardinality(dataset))"
      ],
      "metadata": {
        "id": "YKQ5OBdKfjb8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "y = []\n",
        "extracted_features = []\n",
        "for X_i, attention_map_i, y_i in dataset:\n",
        "    features = phobert(X_i, attention_map_i)\n",
        "    final_features = features[0][:,0,:] \n",
        "    \n",
        "    extracted_features.append(final_features.numpy())\n",
        "    y.append(y_i.numpy())\n",
        "    \n",
        "extracted_features = np.concatenate(extracted_features)\n",
        "y = np.concatenate(y) \n",
        "print(extracted_features.shape, y.shape)"
      ],
      "metadata": {
        "id": "gbRhf1QJgRZb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "with open('data/bert_extracted_review_data.pkl', 'wb') as outfile:\n",
        "    pickle.dump((extracted_features, y), outfile, pickle.HIGHEST_PROTOCOL)"
      ],
      "metadata": {
        "id": "eAvEG9IEQL7j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"data/bert_extracted_review_data.pkl\",\"rb\") as infile:\n",
        "    X,y = pickle.load(infile)"
      ],
      "metadata": {
        "id": "V9SXXW3gQNIC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=1)"
      ],
      "metadata": {
        "id": "e4Jgfpb_QOUx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = LinearSVC(C = 0.005,max_iter=1000)\n",
        "model.fit(X_train, (y_train-1))"
      ],
      "metadata": {
        "id": "i8MxoT_1QQQf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "acc_train = model.score(X_train, y_train-1)\n",
        "acc_val = model.score(X_val, y_val-1)\n",
        "acc_test = model.score(X_test, y_test-1)\n",
        "\n",
        "print(acc_train)\n",
        "print(acc_val)\n",
        "print(acc_test)"
      ],
      "metadata": {
        "id": "I7NEYbROQRYJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = LinearSVC(C=0.01, max_iter=1000)\n",
        "model.fit(X_train, np.where((y_train-1)>=4,0,1))\n",
        "\n",
        "\n",
        "acc_train = model.score(X_train, np.where((y_train-1)>=4,0,1))\n",
        "acc_val = model.score(X_val, np.where((y_val-1)>=4,0,1))\n",
        "acc_test = model.score(X_test, np.where((y_test-1)>=4,0,1))\n",
        "\n",
        "print(acc_train)\n",
        "print(acc_val)\n",
        "print(acc_test)"
      ],
      "metadata": {
        "id": "xml8lfweQSyJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = LogisticRegression(C = 0.03,max_iter=3000, multi_class='multinomial',solver='lbfgs')\n",
        "model.fit(X_train, (y_train-1))"
      ],
      "metadata": {
        "id": "zaz5avcQQUIs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "acc_train = model.score(X_train, y_train-1)\n",
        "acc_val = model.score(X_val, y_val-1)\n",
        "acc_test = model.score(X_test, y_test-1)\n",
        "\n",
        "print(acc_train)\n",
        "print(acc_val)\n",
        "print(acc_test)"
      ],
      "metadata": {
        "id": "iajH31pIQVl7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = RandomForestClassifier()\n",
        "model = model.fit(X_train, y_train-1)"
      ],
      "metadata": {
        "id": "7zuK_SrsQXA0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "acc_train = model.score(X_train, y_train-1)\n",
        "acc_val = model.score(X_val, y_val-1)\n",
        "acc_test = model.score(X_test, y_test-1)\n",
        "\n",
        "print(acc_train)\n",
        "print(acc_val)\n",
        "print(acc_test)"
      ],
      "metadata": {
        "id": "VUEi6tDvQYQe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Input(768),\n",
        "    tf.keras.layers.Dense(256, activation='relu', kernel_regularizer=tf.keras.regularizers.L2(0.0)),\n",
        "    tf.keras.layers.Dense(256, activation='relu', kernel_regularizer=tf.keras.regularizers.L2(0.0)),\n",
        "    tf.keras.layers.Dropout(0.5),\n",
        "    tf.keras.layers.Dense(5, activation ='softmax', kernel_regularizer=tf.keras.regularizers.L2(0.0))\n",
        "])\n",
        "\n",
        "model.compile(optimizer = tf.keras.optimizers.Adam(1e-4),\n",
        "              loss=tf.keras.losses.sparse_categorical_crossentropy,\n",
        "              metrics=['acc'])"
      ],
      "metadata": {
        "id": "j9BnXtD8QZ4R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if not os.path.exists(\"models\"):\n",
        "    os.makedirs(\"models\")\n",
        "\n",
        "mcp_save = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=\"models/bert_nn_classify_model.h5\",\n",
        "    save_weights_only=True,\n",
        "    monitor='val_acc',\n",
        "    mode='max',\n",
        "    save_best_only=True)  \n",
        "\n",
        "history = model.fit(X_train, y_train-1,\n",
        "          epochs=50,\n",
        "          batch_size=32,\n",
        "          validation_data=(X_val, y_val-1),\n",
        "          callbacks=[mcp_save])"
      ],
      "metadata": {
        "id": "bjEK3RJpQboS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_weights(\"models/bert_nn_classify_model.h5\")\n",
        "model.evaluate(X_train, y_train-1)\n",
        "model.evaluate(X_val, y_val-1)\n",
        "model.evaluate(X_test, y_test-1)"
      ],
      "metadata": {
        "id": "XDl3HxQGQeZT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "acc = history.history['acc']\n",
        "val_acc = history.history['val_acc']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "\n",
        "epochs = range(len(acc))\n",
        "\n",
        "plt.plot(epochs, acc, 'r', label='Training accuracy')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation accuracy')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.legend(loc=0)\n",
        "plt.figure()\n",
        "\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "nvAW4TbQQfrE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Metrics():\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def computeMSE(y_true, y_pred):\n",
        "        return np.mean((y_true-y_pred)**2)\n",
        "\n",
        "    @staticmethod\n",
        "    def computeMAE(y_true, y_pred):\n",
        "        return np.mean(np.abs(y_true-y_pred))\n",
        "\n",
        "    @staticmethod\n",
        "    def computeSIA(y_true, y_pred, eps=2):\n",
        "        error = np.abs(y_true - y_pred)\n",
        "        res = np.mean(((error - eps) < 0 ) & (y_pred >= 0))\n",
        "        return res"
      ],
      "metadata": {
        "id": "avnTEH1jQhyQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "total_y_pred = tf.argmax(model.predict(X_test), axis=1)\n",
        "total_y = y_test-1\n",
        "\n",
        "confusion_matrix = tf.math.confusion_matrix(total_y, total_y_pred).numpy()\n",
        "\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import itertools\n",
        "label_dict_test = {i+1:i for i in range(5)}\n",
        "\n",
        "\n",
        "\n",
        "def plot_confusion_matrix(cm, classes,\n",
        "                          normalize=False,\n",
        "                          title='Confusion matrix',\n",
        "                          cmap=plt.cm.Blues):\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1, keepdims = True)\n",
        "\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=45)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "\n",
        "    fmt = '.2f' if normalize else 'd'\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, format(cm[i, j], fmt),\n",
        "                 horizontalalignment=\"center\",\n",
        "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')\n",
        "\n",
        "class_names = label_dict_test.keys()\n",
        "\n",
        "plt.figure(figsize=(10,10))\n",
        "plot_confusion_matrix(confusion_matrix, classes=class_names, normalize=True,\n",
        "                      title= None, cmap= plt.cm.Reds)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "73LFUVsdQkC4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Metrics.computeSIA(total_y, total_y_pred, eps=2)"
      ],
      "metadata": {
        "id": "69Fqra0-Qlsk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}