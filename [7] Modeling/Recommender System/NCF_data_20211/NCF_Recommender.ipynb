{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NCF_Recommender.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "i_PBdPq3Lvf-",
        "85tTVC3BMkCu",
        "dfuOpVT7NmRt",
        "_dwOsB8AOZ69",
        "TkjnwbPqOb4W",
        "o_CcHCEfOdJn",
        "nNc16jbtOejO",
        "pT_DVa6QcvY6",
        "RvVwFJIpdiBm",
        "g1joif4wdnHV",
        "Znvf8nrEdozO",
        "sTxkwo1pPvNQ",
        "IJMTxO7_9-a5",
        "lDt03_Ot9_Wf",
        "9cLU62YG-AQa"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i_PBdPq3Lvf-"
      },
      "source": [
        "# Data Preparation"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!gdown --id 1_-RVgQFxG0Rp2H0Ev0uPoDWjdvcQ3CNv"
      ],
      "metadata": {
        "id": "5peH6pUFcO1k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b-KkZ0o6EOFU"
      },
      "source": [
        "import torch\n",
        "import random, math\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from copy import deepcopy\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "random.seed(1)\n",
        "\n",
        "\n",
        "class UserItemRatingDataset(Dataset):\n",
        "    \"\"\"Wrapper, convert <user, item, rating> Tensor into Pytorch Dataset\"\"\"\n",
        "    def __init__(self, user_tensor, item_tensor, target_tensor):\n",
        "        self.user_tensor = user_tensor\n",
        "        self.item_tensor = item_tensor\n",
        "        self.target_tensor = target_tensor\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return self.user_tensor[index], self.item_tensor[index], self.target_tensor[index]\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.user_tensor.size(0)\n",
        "\n",
        "\n",
        "class SampleGenerator(object):\n",
        "    \"\"\"Construct dataset for NCF\"\"\"\n",
        "    def __init__(self, ratings):\n",
        "        \"\"\"\n",
        "        args:\n",
        "            ratings: pd.DataFrame, which contains 4 columns = ['userId', 'itemId', 'rating', 'timestamp']\n",
        "        \"\"\"\n",
        "        assert 'userId' in ratings.columns\n",
        "        assert 'itemId' in ratings.columns\n",
        "        assert 'rating' in ratings.columns\n",
        "\n",
        "        self.ratings = ratings\n",
        "        self.preprocess_ratings = self._binarize(ratings)\n",
        "        self.user_pool = set(self.ratings['userId'].unique())\n",
        "        self.item_pool = set(self.ratings['itemId'].unique())\n",
        "        self.negatives = self._sample_negative(ratings)\n",
        "        self.train_ratings, self.test_ratings = self._split_loo(self.preprocess_ratings)\n",
        "\n",
        "    def _normalize(self, ratings):\n",
        "        \"\"\"normalize into [0, 1] from [0, max_rating], explicit feedback\"\"\"\n",
        "        ratings = deepcopy(ratings)\n",
        "        max_rating = ratings.rating.max()\n",
        "        ratings['rating'] = ratings.rating * 1.0 / max_rating\n",
        "        return ratings\n",
        "\n",
        "    def _binarize(self, ratings):\n",
        "        \"\"\"binarize into 0 or 1, imlicit feedback\"\"\"\n",
        "        ratings = deepcopy(ratings)\n",
        "        ratings['rating'][ratings['rating'] > 0] = 1.0\n",
        "        return ratings\n",
        "\n",
        "    def _split_loo(self, ratings):\n",
        "        \"\"\"leave one out train/test split \"\"\"\n",
        "        ratings['rank_latest'] = ratings.groupby(['userId'])['timestamp'].rank(method='first', ascending=False)\n",
        "        test = ratings[ratings['rank_latest'] == 1]\n",
        "        train = ratings[ratings['rank_latest'] > 1]\n",
        "        assert train['userId'].nunique() == test['userId'].nunique()\n",
        "        return train[['userId', 'itemId', 'rating']], test[['userId', 'itemId', 'rating']]\n",
        "\n",
        "    def _sample_negative(self, ratings):\n",
        "        \"\"\"return all negative items & 99 sampled negative items\"\"\"\n",
        "        interact_status = ratings.groupby('userId')['itemId'].apply(set).reset_index().rename(\n",
        "            columns={'itemId': 'interacted_items'})\n",
        "        interact_status['negative_items'] = interact_status['interacted_items'].apply(lambda x: self.item_pool - x)\n",
        "        interact_status['negative_samples'] = interact_status['negative_items'].apply(lambda x: random.sample(x, 99))\n",
        "        return interact_status[['userId', 'negative_items', 'negative_samples']]\n",
        "\n",
        "    def instance_a_train_loader(self, num_negatives, batch_size):\n",
        "        \"\"\"instance train loader for one training epoch\"\"\"\n",
        "        users, items, ratings = [], [], []\n",
        "        train_ratings = pd.merge(self.train_ratings, self.negatives[['userId', 'negative_items']], on='userId')\n",
        "        train_ratings['negatives'] = train_ratings['negative_items'].apply(lambda x: random.sample(x, num_negatives))\n",
        "        for row in train_ratings.itertuples():\n",
        "            users.append(int(row.userId))\n",
        "            items.append(int(row.itemId))\n",
        "            ratings.append(float(row.rating))\n",
        "            for i in range(num_negatives):\n",
        "                users.append(int(row.userId))\n",
        "                items.append(int(row.negatives[i]))\n",
        "                ratings.append(float(0)) \n",
        "        dataset = UserItemRatingDataset(user_tensor=torch.LongTensor(users),\n",
        "                                        item_tensor=torch.LongTensor(items),\n",
        "                                        target_tensor=torch.FloatTensor(ratings))\n",
        "        return DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    @property\n",
        "    def evaluate_data(self):\n",
        "        \"\"\"create evaluate data\"\"\"\n",
        "        test_ratings = pd.merge(self.test_ratings, self.negatives[['userId', 'negative_samples']], on='userId')\n",
        "        test_users, test_items, negative_users, negative_items = [], [], [], []\n",
        "        for row in test_ratings.itertuples():\n",
        "            test_users.append(int(row.userId))\n",
        "            test_items.append(int(row.itemId))\n",
        "            for i in range(len(row.negative_samples)):\n",
        "                negative_users.append(int(row.userId))\n",
        "                negative_items.append(int(row.negative_samples[i]))\n",
        "        return [torch.LongTensor(test_users), torch.LongTensor(test_items), torch.LongTensor(negative_users),\n",
        "                torch.LongTensor(negative_items)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "85tTVC3BMkCu"
      },
      "source": [
        "# Utils"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "upWcLcSdMdP0"
      },
      "source": [
        "def save_checkpoint(model, model_dir):\n",
        "    torch.save(model.state_dict(), model_dir)\n",
        "\n",
        "def resume_checkpoint(model, model_dir, device_id):\n",
        "    state_dict = torch.load(model_dir,\n",
        "                            map_location=lambda storage, loc: storage.cuda(device=device_id))  # ensure all storage are on gpu\n",
        "    model.load_state_dict(state_dict)\n",
        "\n",
        "def use_cuda(enabled, device_id=0):\n",
        "    if enabled:\n",
        "        assert torch.cuda.is_available(), 'CUDA is not available'\n",
        "        torch.cuda.set_device(device_id)\n",
        "\n",
        "def use_optimizer(network, params):\n",
        "    if params['optimizer'] == 'sgd':\n",
        "        optimizer = torch.optim.SGD(network.parameters(),\n",
        "                                    lr=params['sgd_lr'],\n",
        "                                    momentum=params['sgd_momentum'],\n",
        "                                    weight_decay=params['l2_regularization'])\n",
        "    elif params['optimizer'] == 'adam':\n",
        "        optimizer = torch.optim.Adam(network.parameters(), \n",
        "                                     lr=params['adam_lr'],\n",
        "                                     weight_decay=params['l2_regularization'])\n",
        "    elif params['optimizer'] == 'rmsprop':\n",
        "        optimizer = torch.optim.RMSprop(network.parameters(),\n",
        "                                        lr=params['rmsprop_lr'],\n",
        "                                        alpha=params['rmsprop_alpha'],\n",
        "                                        momentum=params['rmsprop_momentum'])\n",
        "    return optimizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "00GmKpoANC1A"
      },
      "source": [
        "class EvalMetricTopK(object):\n",
        "    def __init__(self, top_k):\n",
        "        self._top_k = top_k\n",
        "        self._subjects = None \n",
        "\n",
        "    @property\n",
        "    def top_k(self):\n",
        "        return self._top_k\n",
        "\n",
        "    @top_k.setter\n",
        "    def top_k(self, top_k):\n",
        "        self._top_k = top_k\n",
        "\n",
        "    @property\n",
        "    def subjects(self):\n",
        "        return self._subjects\n",
        "\n",
        "    @subjects.setter\n",
        "    def subjects(self, subjects):\n",
        "        \"\"\"\n",
        "        args:\n",
        "            subjects: list, [test_users, test_items, test_scores, negative users, negative items, negative scores]\n",
        "        \"\"\"\n",
        "        assert isinstance(subjects, list)\n",
        "        test_users, test_items, test_scores = subjects[0], subjects[1], subjects[2]\n",
        "        neg_users, neg_items, neg_scores = subjects[3], subjects[4], subjects[5]\n",
        "       \n",
        "        test = pd.DataFrame({'user': test_users,\n",
        "                             'test_item': test_items,\n",
        "                             'test_score': test_scores})\n",
        "        full = pd.DataFrame({'user': neg_users + test_users,\n",
        "                            'item': neg_items + test_items,\n",
        "                            'score': neg_scores + test_scores})\n",
        "        full = pd.merge(full, test, on=['user'], how='left')\n",
        "        full['rank'] = full.groupby('user')['score'].rank(method='first', ascending=False)\n",
        "        full.sort_values(['user', 'rank'], inplace=True)\n",
        "        self._subjects = full\n",
        "\n",
        "    def cal_hit_ratio(self):\n",
        "        \"\"\"Hit Ratio @ top_K\"\"\"\n",
        "        full, top_k = self._subjects, self._top_k\n",
        "        top_k = full[full['rank']<=top_k]\n",
        "        test_in_top_k =top_k[top_k['test_item'] == top_k['item']]  \n",
        "        return len(test_in_top_k) * 1.0 / full['user'].nunique()\n",
        "\n",
        "    def cal_ndcg(self):\n",
        "        full, top_k = self._subjects, self._top_k\n",
        "        top_k = full[full['rank']<=top_k]\n",
        "        test_in_top_k =top_k[top_k['test_item'] == top_k['item']]\n",
        "        test_in_top_k['ndcg'] = test_in_top_k['rank'].apply(lambda x: math.log(2) / math.log(1 + x))\n",
        "        return test_in_top_k['ndcg'].sum() * 1.0 / full['user'].nunique()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dfuOpVT7NmRt"
      },
      "source": [
        "## Engine"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7VBJecl-No6p"
      },
      "source": [
        "from torch.autograd import Variable\n",
        "\n",
        "class Engine(object):\n",
        "    \"\"\"Meta Engine for training & evaluating NCF model\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config):\n",
        "        self.config = config \n",
        "        self._metron = EvalMetricTopK(top_k=10)\n",
        "        self.opt = use_optimizer(self.model, config)\n",
        "        self.crit = torch.nn.BCELoss()\n",
        "\n",
        "    def train_single_batch(self, users, items, ratings):\n",
        "        assert hasattr(self, 'model'), 'Please specify the exact model !'\n",
        "        if self.config['use_cuda'] is True:\n",
        "            users, items, ratings = users.cuda(), items.cuda(), ratings.cuda()\n",
        "        self.opt.zero_grad()\n",
        "        ratings_pred = self.model(users, items)\n",
        "        loss = self.crit(ratings_pred.view(-1), ratings)\n",
        "        loss.backward()\n",
        "        self.opt.step()\n",
        "        loss = loss.item()\n",
        "        return loss\n",
        "\n",
        "    def train_an_epoch(self, train_loader, epoch_id):\n",
        "        assert hasattr(self, 'model'), 'Please specify the exact model !'\n",
        "        self.model.train()\n",
        "        total_loss = 0\n",
        "        for batch_id, batch in enumerate(train_loader):\n",
        "            assert isinstance(batch[0], torch.LongTensor)\n",
        "            user, item, rating = batch[0], batch[1], batch[2]\n",
        "            rating = rating.float()\n",
        "            loss = self.train_single_batch(user, item, rating)\n",
        "            total_loss += loss\n",
        "\n",
        "    def evaluate(self, evaluate_data, epoch_id):\n",
        "        assert hasattr(self, 'model'), 'Please specify the exact model !'\n",
        "        self.model.eval()\n",
        "        with torch.no_grad():\n",
        "            test_users, test_items = evaluate_data[0], evaluate_data[1]\n",
        "            negative_users, negative_items = evaluate_data[2], evaluate_data[3]\n",
        "            if self.config['use_cuda'] is True:\n",
        "                test_users = test_users.cuda()\n",
        "                test_items = test_items.cuda()\n",
        "                negative_users = negative_users.cuda()\n",
        "                negative_items = negative_items.cuda()\n",
        "            test_scores = self.model(test_users, test_items)\n",
        "            negative_scores = self.model(negative_users, negative_items)\n",
        "            if self.config['use_cuda'] is True:\n",
        "                test_users = test_users.cpu()\n",
        "                test_items = test_items.cpu()\n",
        "                test_scores = test_scores.cpu()\n",
        "                negative_users = negative_users.cpu()\n",
        "                negative_items = negative_items.cpu()\n",
        "                negative_scores = negative_scores.cpu()\n",
        "            self._metron.subjects = [test_users.data.view(-1).tolist(),\n",
        "                                 test_items.data.view(-1).tolist(),\n",
        "                                 test_scores.data.view(-1).tolist(),\n",
        "                                 negative_users.data.view(-1).tolist(),\n",
        "                                 negative_items.data.view(-1).tolist(),\n",
        "                                 negative_scores.data.view(-1).tolist()]\n",
        "        hit_ratio, ndcg = self._metron.cal_hit_ratio(), self._metron.cal_ndcg()\n",
        "        print('[Evluating Epoch {}] HR = {:.4f}, NDCG = {:.4f}'.format(epoch_id, hit_ratio, ndcg))\n",
        "        return hit_ratio, ndcg\n",
        "\n",
        "    def save(self, alias, epoch_id, hit_ratio, ndcg):\n",
        "        assert hasattr(self, 'model'), 'Please specify the exact model !'\n",
        "        model_dir = self.config['model_dir'].format(alias, epoch_id, hit_ratio, ndcg)\n",
        "        save_checkpoint(self.model, model_dir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_dwOsB8AOZ69"
      },
      "source": [
        "# Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TkjnwbPqOb4W"
      },
      "source": [
        "## GMF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sH2EK18MOgAW"
      },
      "source": [
        "class GMF(torch.nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(GMF, self).__init__()\n",
        "        self.num_users = config['num_users']\n",
        "        self.num_items = config['num_items']\n",
        "        self.latent_dim = config['latent_dim']\n",
        "\n",
        "        self.embedding_user = torch.nn.Embedding(num_embeddings=self.num_users, embedding_dim=self.latent_dim)\n",
        "        self.embedding_item = torch.nn.Embedding(num_embeddings=self.num_items, embedding_dim=self.latent_dim)\n",
        "\n",
        "        self.affine_output = torch.nn.Linear(in_features=self.latent_dim, out_features=1)\n",
        "        self.logistic = torch.nn.Sigmoid()\n",
        "\n",
        "    def forward(self, user_indices, item_indices):\n",
        "        user_embedding = self.embedding_user(user_indices)\n",
        "        item_embedding = self.embedding_item(item_indices)\n",
        "        element_product = torch.mul(user_embedding, item_embedding)\n",
        "        logits = self.affine_output(element_product)\n",
        "        rating = self.logistic(logits)\n",
        "        return rating\n",
        "\n",
        "    def init_weight(self):\n",
        "        pass\n",
        "\n",
        "\n",
        "class GMFEngine(Engine):\n",
        "    \"\"\"Engine for training & evaluating GMF model\"\"\"\n",
        "    def __init__(self, config):\n",
        "        self.model = GMF(config)\n",
        "        if config['use_cuda'] is True:\n",
        "            use_cuda(True, config['device_id'])\n",
        "            self.model.cuda()\n",
        "        super(GMFEngine, self).__init__(config)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o_CcHCEfOdJn"
      },
      "source": [
        "## MLP"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HTZF8V8vOgoX"
      },
      "source": [
        "class MLP(torch.nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(MLP, self).__init__()\n",
        "        self.config = config\n",
        "        self.num_users = config['num_users']\n",
        "        self.num_items = config['num_items']\n",
        "        self.latent_dim = config['latent_dim']\n",
        "\n",
        "        self.embedding_user = torch.nn.Embedding(num_embeddings=self.num_users, embedding_dim=self.latent_dim)\n",
        "        self.embedding_item = torch.nn.Embedding(num_embeddings=self.num_items, embedding_dim=self.latent_dim)\n",
        "\n",
        "        self.fc_layers = torch.nn.ModuleList()\n",
        "        for idx, (in_size, out_size) in enumerate(zip(config['layers'][:-1], config['layers'][1:])):\n",
        "            self.fc_layers.append(torch.nn.Linear(in_size, out_size))\n",
        "\n",
        "        self.affine_output = torch.nn.Linear(in_features=config['layers'][-1], out_features=1)\n",
        "        self.logistic = torch.nn.Sigmoid()\n",
        "\n",
        "    def forward(self, user_indices, item_indices):\n",
        "        user_embedding = self.embedding_user(user_indices)\n",
        "        item_embedding = self.embedding_item(item_indices)\n",
        "        vector = torch.cat([user_embedding, item_embedding], dim=-1)  # the concat latent vector\n",
        "        for idx, _ in enumerate(range(len(self.fc_layers))):\n",
        "            vector = self.fc_layers[idx](vector)\n",
        "            vector = torch.nn.ReLU()(vector)\n",
        "        logits = self.affine_output(vector)\n",
        "        rating = self.logistic(logits)\n",
        "        return rating\n",
        "\n",
        "    def init_weight(self):\n",
        "        pass\n",
        "\n",
        "    def load_pretrain_weights(self):\n",
        "        \"\"\"Loading weights from trained GMF model\"\"\"\n",
        "        config = self.config\n",
        "        gmf_model = GMF(config)\n",
        "        if config['use_cuda'] is True:\n",
        "            gmf_model.cuda()\n",
        "        resume_checkpoint(gmf_model, model_dir=config['pretrain_mf'], device_id=config['device_id'])\n",
        "        self.embedding_user.weight.data = gmf_model.embedding_user.weight.data\n",
        "        self.embedding_item.weight.data = gmf_model.embedding_item.weight.data\n",
        "\n",
        "class MLPEngine(Engine):\n",
        "    \"\"\"Engine for training & evaluating GMF model\"\"\"\n",
        "    def __init__(self, config):\n",
        "        self.model = MLP(config)\n",
        "        if config['use_cuda'] is True:\n",
        "            use_cuda(True, config['device_id'])\n",
        "            self.model.cuda()\n",
        "        super(MLPEngine, self).__init__(config)\n",
        "        print(self.model)\n",
        "        if config['pretrain']:\n",
        "            self.model.load_pretrain_weights()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nNc16jbtOejO"
      },
      "source": [
        "## NMF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lh6EXlUSOhIg"
      },
      "source": [
        "class NMF(torch.nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(NMF, self).__init__()\n",
        "        self.config = config\n",
        "        self.num_users = config['num_users']\n",
        "        self.num_items = config['num_items']\n",
        "        self.latent_dim_mf = config['latent_dim_mf']\n",
        "        self.latent_dim_mlp = config['latent_dim_mlp']\n",
        "\n",
        "        self.embedding_user_mlp = torch.nn.Embedding(num_embeddings=self.num_users, embedding_dim=self.latent_dim_mlp)\n",
        "        self.embedding_item_mlp = torch.nn.Embedding(num_embeddings=self.num_items, embedding_dim=self.latent_dim_mlp)\n",
        "        self.embedding_user_mf = torch.nn.Embedding(num_embeddings=self.num_users, embedding_dim=self.latent_dim_mf)\n",
        "        self.embedding_item_mf = torch.nn.Embedding(num_embeddings=self.num_items, embedding_dim=self.latent_dim_mf)\n",
        "\n",
        "        self.fc_layers = torch.nn.ModuleList()\n",
        "        for idx, (in_size, out_size) in enumerate(zip(config['layers'][:-1], config['layers'][1:])):\n",
        "            self.fc_layers.append(torch.nn.Linear(in_size, out_size))\n",
        "\n",
        "        self.affine_output = torch.nn.Linear(in_features=config['layers'][-1] + config['latent_dim_mf'], out_features=1)\n",
        "        self.logistic = torch.nn.Sigmoid()\n",
        "\n",
        "    def forward(self, user_indices, item_indices):\n",
        "        user_embedding_mlp = self.embedding_user_mlp(user_indices)\n",
        "        item_embedding_mlp = self.embedding_item_mlp(item_indices)\n",
        "        user_embedding_mf = self.embedding_user_mf(user_indices)\n",
        "        item_embedding_mf = self.embedding_item_mf(item_indices)\n",
        "\n",
        "        mlp_vector = torch.cat([user_embedding_mlp, item_embedding_mlp], dim=-1)  # the concat latent vector\n",
        "        mf_vector =torch.mul(user_embedding_mf, item_embedding_mf)\n",
        "\n",
        "        for idx, _ in enumerate(range(len(self.fc_layers))):\n",
        "            mlp_vector = self.fc_layers[idx](mlp_vector)\n",
        "            mlp_vector = torch.nn.ReLU()(mlp_vector)\n",
        "\n",
        "        vector = torch.cat([mlp_vector, mf_vector], dim=-1)\n",
        "        logits = self.affine_output(vector)\n",
        "        rating = self.logistic(logits)\n",
        "        return rating\n",
        "\n",
        "    def init_weight(self):\n",
        "        pass\n",
        "\n",
        "    def load_pretrain_weights(self):\n",
        "        \"\"\"Loading weights from trained MLP model & GMF model\"\"\"\n",
        "        config = self.config\n",
        "        config['latent_dim'] = config['latent_dim_mlp']\n",
        "        mlp_model = MLP(config)\n",
        "        if config['use_cuda'] is True:\n",
        "            mlp_model.cuda()\n",
        "        resume_checkpoint(mlp_model, model_dir=config['pretrain_mlp'], device_id=config['device_id'])\n",
        "\n",
        "        self.embedding_user_mlp.weight.data = mlp_model.embedding_user.weight.data\n",
        "        self.embedding_item_mlp.weight.data = mlp_model.embedding_item.weight.data\n",
        "        for idx in range(len(self.fc_layers)):\n",
        "            self.fc_layers[idx].weight.data = mlp_model.fc_layers[idx].weight.data\n",
        "\n",
        "        config['latent_dim'] = config['latent_dim_mf']\n",
        "        gmf_model = GMF(config)\n",
        "        if config['use_cuda'] is True:\n",
        "            gmf_model.cuda()\n",
        "        resume_checkpoint(gmf_model, model_dir=config['pretrain_mf'], device_id=config['device_id'])\n",
        "        self.embedding_user_mf.weight.data = gmf_model.embedding_user.weight.data\n",
        "        self.embedding_item_mf.weight.data = gmf_model.embedding_item.weight.data\n",
        "\n",
        "        self.affine_output.weight.data = 0.5 * torch.cat([mlp_model.affine_output.weight.data, gmf_model.affine_output.weight.data], dim=-1)\n",
        "        self.affine_output.bias.data = 0.5 * (mlp_model.affine_output.bias.data + gmf_model.affine_output.bias.data)\n",
        "\n",
        "\n",
        "class NMFEngine(Engine):\n",
        "    \"\"\"Engine for training & evaluating GMF model\"\"\"\n",
        "    def __init__(self, config):\n",
        "        self.model = NMF(config)\n",
        "        if config['use_cuda'] is True:\n",
        "            use_cuda(True, config['device_id'])\n",
        "            self.model.cuda()\n",
        "        super(NMFEngine, self).__init__(config)\n",
        "        print(self.model)\n",
        "\n",
        "        if config['pretrain']:\n",
        "            self.model.load_pretrain_weights()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Config"
      ],
      "metadata": {
        "id": "pT_DVa6QcvY6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GMF"
      ],
      "metadata": {
        "id": "RvVwFJIpdiBm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gmf_config = {'alias': 'gmf_implict',\n",
        "              'num_epoch': 50,\n",
        "              'batch_size': 512,\n",
        "              'optimizer': 'adam',\n",
        "              'adam_lr': 1e-1,\n",
        "              'num_users': 3405,\n",
        "              'num_items': 11312,\n",
        "              'latent_dim': 8,\n",
        "              'num_negative': 4,\n",
        "              'l2_regularization': 0, \n",
        "              'use_cuda': True,\n",
        "              'device_id': 0,\n",
        "              'model_dir':'./{}_Epoch{}_HR{:.4f}_NDCG{:.4f}.model'}"
      ],
      "metadata": {
        "id": "3v6spWSwdksk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MLP"
      ],
      "metadata": {
        "id": "g1joif4wdnHV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mlp_config = {'alias': 'mlp_implicit',\n",
        "              'num_epoch': 50,\n",
        "              'batch_size': 512, \n",
        "              'optimizer': 'adam',\n",
        "              'adam_lr': 1e-2,\n",
        "              'num_users': 3405,\n",
        "              'num_items': 11312,\n",
        "              'latent_dim': 32,\n",
        "              'num_negative': 4,\n",
        "              'layers': [64,128,64,32,16],  \n",
        "              'l2_regularization':0,\n",
        "              'use_cuda': True,\n",
        "              'device_id': 0,\n",
        "              'pretrain': False,\n",
        "              'pretrain_mf': '',\n",
        "              'model_dir':'./{}_Epoch{}_HR{:.4f}_NDCG{:.4f}.model'}"
      ],
      "metadata": {
        "id": "WFVi7TsrdoIu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## NMF"
      ],
      "metadata": {
        "id": "Znvf8nrEdozO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nmf_config = {'alias': 'nmf_implicit',\n",
        "                'num_epoch': 50,\n",
        "                'batch_size': 512,\n",
        "                'optimizer': 'adam',\n",
        "                'adam_lr': 1e-2,\n",
        "                'num_users': 3405,\n",
        "                'num_items': 11312,\n",
        "                'latent_dim_mf': 16,\n",
        "                'latent_dim_mlp': 16,\n",
        "                'num_negative': 4,\n",
        "                'layers': [32,64,32,16], \n",
        "                'l2_regularization': 0,\n",
        "                'use_cuda': True,\n",
        "                'device_id': 0,\n",
        "                'pretrain': False,\n",
        "                'pretrain_mf': '',\n",
        "                'pretrain_mlp': '',\n",
        "                'model_dir':'./{}_Epoch{}_HR{:.4f}_NDCG{:.4f}.model'\n",
        "                }"
      ],
      "metadata": {
        "id": "zVIqWIyocxjG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sTxkwo1pPvNQ"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load Data"
      ],
      "metadata": {
        "id": "dqyzhjmR7WNy"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ChwmnYmSQJJY",
        "outputId": "b429a1d7-37cd-40fd-a741-3fbe37fad395"
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "# Load Data\n",
        "dir = './tiki_rating_10.csv'\n",
        "rating_data = pd.read_csv(dir)\n",
        "sample_generator = SampleGenerator(ratings=rating_data)\n",
        "evaluate_data = sample_generator.evaluate_data\n",
        "print('Range of userId is [{}, {}]'.format(rating_data.userId.min(), rating_data.userId.max()))\n",
        "print('Range of itemId is [{}, {}]'.format(rating_data.itemId.min(), rating_data.itemId.max()))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Range of userId is [0, 3404]\n",
            "Range of itemId is [0, 11311]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### GMF"
      ],
      "metadata": {
        "id": "IJMTxO7_9-a5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "config = gmf_config\n",
        "engine = GMFEngine(config)\n",
        "for epoch in range(config['num_epoch']):\n",
        "    print('Epoch {} starts !'.format(epoch))\n",
        "    print('-' * 80)\n",
        "    train_loader = sample_generator.instance_a_train_loader(config['num_negative'], config['batch_size'])\n",
        "    engine.train_an_epoch(train_loader, epoch_id=epoch)\n",
        "    hit_ratio, ndcg = engine.evaluate(evaluate_data, epoch_id=epoch)\n",
        "    engine.save(config['alias'], epoch, hit_ratio, ndcg)"
      ],
      "metadata": {
        "id": "2AG9Qwg0-FBh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### MLP"
      ],
      "metadata": {
        "id": "lDt03_Ot9_Wf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "config = mlp_config\n",
        "engine = MLPEngine(config)\n",
        "for epoch in range(config['num_epoch']):\n",
        "    print('Epoch {} starts !'.format(epoch))\n",
        "    print('-' * 80)\n",
        "    train_loader = sample_generator.instance_a_train_loader(config['num_negative'], config['batch_size'])\n",
        "    engine.train_an_epoch(train_loader, epoch_id=epoch)\n",
        "    hit_ratio, ndcg = engine.evaluate(evaluate_data, epoch_id=epoch)\n",
        "    engine.save(config['alias'], epoch, hit_ratio, ndcg)"
      ],
      "metadata": {
        "id": "amBX50sa-HUt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### NMF"
      ],
      "metadata": {
        "id": "9cLU62YG-AQa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "config = nmf_config\n",
        "engine = NMFEngine(config)\n",
        "for epoch in range(config['num_epoch']):\n",
        "    print('Epoch {} starts !'.format(epoch))\n",
        "    print('-' * 80)\n",
        "    train_loader = sample_generator.instance_a_train_loader(config['num_negative'], config['batch_size'])\n",
        "    engine.train_an_epoch(train_loader, epoch_id=epoch)\n",
        "    hit_ratio, ndcg = engine.evaluate(evaluate_data, epoch_id=epoch)\n",
        "    engine.save(config['alias'], epoch, hit_ratio, ndcg)"
      ],
      "metadata": {
        "id": "LBYtc7A5-I61"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}